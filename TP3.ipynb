{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c441f82",
   "metadata": {},
   "source": [
    "# Trabajo Práctico N°3. Regresión Lineal\n",
    "\n",
    "## Parte I\n",
    "Del libro “Bishop, C. Pattern Recognition and machine learning” resolver los ejercicios 3.12, 3.13, 3.23, y 3.24."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceda167b",
   "metadata": {},
   "source": [
    "### Ejercicio 3.12\n",
    "\n",
    "Vimos en la sección 2.3.6 que el conjugado a priori para una distribución gaussiana con media desconocida y varianza desconocida es una distribución normal-gamma. Esta propiedad también se mantiene para el caso de la distribución gaussiana condicional $p(t|\\mathbf{x}, \\mathbf{w}, \\beta)$ del modelo de regresión lineal. Si consideramos la función de verosimilitud (3.10), entonces el conjugado a priori para $\\mathbf{w}$ y $\\beta$ es \n",
    "\n",
    "$$ p(\\mathbf{w}, \\beta) = \\mathcal{N}(\\mathbf{w}|\\mathbf{m}_0, \\beta^{-1}\\mathbf{S}_0)\\text{Gam}(\\beta|a_0, b_0) \\tag{3.112}$$\n",
    "\n",
    "Demuestre que la distribución a posteriori tiene la misma forma funcional, es decir,\n",
    "\n",
    "$$ p(\\mathbf{w}, \\beta|t) = \\mathcal{N}(\\mathbf{w}|\\mathbf{m}_N, \\beta^{-1}\\mathbf{S}_N)\\text{Gam}(\\beta|a_N, b_N) \\tag{3.113}$$\n",
    "\n",
    "y encuentre las expresiones para los parámetros a posteriori $\\mathbf{m}_N$, $\\mathbf{S}_N$, $a_N$, $b_N$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae6e77e",
   "metadata": {},
   "source": [
    "Este ejercicio trata sobre la inferencia bayesiana completa para el modelo de regresión lineal con funciones base, donde tanto el vector de pesos $\\mathbf{w}$ como la precisión del ruido $\\beta$ son tratados como variables aleatorias desconocidas.\n",
    "\n",
    "Comencemos por escribir la función de densidad de probabilidad a priori $p(\\mathbf{w},\\beta)$:\n",
    "\n",
    "\\begin{align*}\n",
    "p(\\mathbf{w},\\beta) &= \\mathcal{N}(\\mathbf{w}|\\mathbf{m}_0,\\beta^{-1}\\mathbf{S}_0) \\text{Gam}(\\beta|a_0,b_0) \\qquad (*)\n",
    "\\end{align*}\n",
    "expandimos las definiciones de las dos distribuciones que componen la distribución Normal-Gamma\n",
    "\\begin{align*}\n",
    "p(\\mathbf{w},\\beta) &\\propto \\left(\\frac{\\beta}{|\\mathbf{S}_0|}\\right)^2 \\text{exp}(-\\frac{1}{2}(\\mathbf{w}-\\mathbf{m}_0)^T \\beta\\mathbf{S}_0^{-1}(\\mathbf{w}-\\mathbf{m}_0))b_0^{a_0}\\beta^{a_0-1}\\text{exp}(-b_0\\beta)\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Y luego escribimos la función de verosimilitud $p(\\mathbf{t}|\\mathbf{X},\\mathbf{w},\\beta)$ :\n",
    "\n",
    "\\begin{align*}\n",
    "p(\\mathbf{t}|\\mathbf{X},\\mathbf{w},\\beta) &= \\prod_{n=1}^{N}\\mathcal{N}(t_n|\\mathbf{w}^T\\mathbf{\\Phi}(\\mathbf{x}_n),\\beta^{-1})\\\\\n",
    "&\\propto \\prod_{n=1}^{N}\\beta^{1/2}\\text{exp}\\left[-\\frac{\\beta}{2}(t_n-\\mathbf{w}^T\\mathbf{\\Phi}(\\mathbf{x}_n))^2\\right] \\qquad (**)\\\\\n",
    "\n",
    "\\end{align*}\n",
    "\n",
    "Según la inferencia bayesiana, tenemos $p(\\mathbf{w},\\beta|\\mathbf{t}) \\propto p(\\mathbf{t}|\\mathbf{X},\\mathbf{w},\\beta)\\times p(\\mathbf{w},\\beta)$.  \n",
    "Nos enfocamos en el término cuadrático con respecto a $\\mathbf{w}$ en el exponente.\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{término cuadrático} &= -\\frac{\\beta}{2}\\mathbf{w}^T\\mathbf{S}_0^{-1}\\mathbf{w} + \\sum_{n=1}^{N}-\\frac{\\beta}{2}\\mathbf{w}^T\\mathbf{\\Phi}(\\mathbf{x}_n)\\mathbf{\\Phi}(\\mathbf{x}_n)^T\\mathbf{w} \\\\\n",
    "&= -\\frac{\\beta}{2}\\mathbf{w}^T\\left[\\mathbf{S}_0^{-1} + \\sum_{n=1}^{N}\\mathbf{\\Phi}(\\mathbf{x}_n)\\mathbf{\\Phi}(\\mathbf{x}_n)^T\\right]\\mathbf{w} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Donde el primer término es generado por (*), y el segundo por (**). Por ahora, sabemos que:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{S}_N^{-1} = \\mathbf{S}_0^{-1} + \\sum_{n=1}^{N}\\mathbf{\\Phi}(\\mathbf{x}_n)\\mathbf{\\Phi}(\\mathbf{x}_n)^T\n",
    "\\end{align*}\n",
    "\n",
    "Luego nos enfocamos en el término lineal con respecto a $\\mathbf{w}$ en el exponente.\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{término lineal} &= \\beta\\mathbf{m_0}^T\\mathbf{S}_0^{-1}\\mathbf{w} + \\sum_{n=1}^{N}\\beta t_n\\mathbf{\\Phi}(\\mathbf{x}_n)^T\\mathbf{w} \\\\\n",
    "&= \\beta\\left[\\mathbf{m_0}^T\\mathbf{S}_0^{-1}+\\sum_{n=1}^{N}t_n\\mathbf{\\Phi}(\\mathbf{x}_n)^T\\right]\\mathbf{w} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Nuevamente, el primer término es generado por (*), y el segundo por (**). Podemos también obtener:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{m}_N^T\\mathbf{S}_N^{-1} = \\mathbf{m}_0^T\\mathbf{S}_0^{-1} + \\sum_{n=1}^{N}t_n\\mathbf{\\Phi}(\\mathbf{x}_n)^T\n",
    "\\end{align*}\n",
    "\n",
    "Lo que nos da:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{m}_N = \\mathbf{S}_N\\left[\\mathbf{S}_0^{-1}\\mathbf{m}_0 + \\sum_{n=1}^{N}t_n\\mathbf{\\Phi}(\\mathbf{x}_n)\\right]\n",
    "\\end{align*}\n",
    "\n",
    "$a_N$ (Parámetro de Forma de la Gamma):\n",
    "\n",
    "$a_N$ se incrementa con la mitad del número de observaciones $N$. Este resultado es análogo al encontrado cuando se infiere solo la precisión:\n",
    "$$a_N = a_0 + \\frac{N}{2}$$\n",
    "\n",
    "$b_N$ (Parámetro de Escala Inversa de la Gamma):\n",
    "\n",
    "$b_N$ (que corresponde a la inversa del parámetro de escala de la distribución Gamma) se actualiza sumando el valor de la priori $b_0$ más un término que representa el error cuadrático residual (residual sum of squares) después de proyectar los datos sobre la media a posteriori:\n",
    "$$b_N = b_0 + \\frac{1}{2} \\left( \\mathbf{m}_0^T \\mathbf{S}_0^{-1} \\mathbf{m}_0 - \\mathbf{m}_N^T \\mathbf{S}_N^{-1} \\mathbf{m}_N + \\sum_{n=1}^N t_n^2 \\right)$$\n",
    "La suma $\\displaystyle\\sum_{n=1}^N t_n^2$ se puede escribir como $\\mathbf{t}^T \\mathbf{t}$.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e00670",
   "metadata": {},
   "source": [
    "### Ejercicio 3.13\n",
    "\n",
    "Demuestre que la distribución predictiva $p(t|\\mathbf{x}, \\mathbf{t})$ para el modelo discutido en el ejercicio 3.12 es una distribución t de Student de la forma\n",
    "$$ p(t|\\mathbf{x}, \\mathbf{t}) = \\text{St}(t|\\mu, \\lambda, \\nu) \\tag{3.114}$$\n",
    "y encuentre las expresiones para $\\mu$, $\\lambda$ y $\\nu$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025cbca4",
   "metadata": {},
   "source": [
    "Este ejercicio requiere demostrar que la distribución predictiva en un modelo de regresión lineal Bayesiano, donde tanto los pesos $\\mathbf{w}$ como la precisión del ruido $\\beta$ son inciertos (modelo discutido en el Ejercicio 3.12), es una distribución $t$ de Student.\n",
    "\n",
    "La demostración se basa en el principio de marginalizar la distribución condicional (verosimilitud) sobre la distribución a posteriori conjunta de los parámetros. Dado que tanto $\\mathbf{w}$ como $\\beta$ son desconocidos, la distribución predictiva se calcula mediante la integral:\n",
    "\n",
    "$$ p(t|\\mathbf{x}, \\mathbf{t}) = \\iint p(t|\\mathbf{x}, \\mathbf{w}, \\beta) p(\\mathbf{w}, \\beta|\\mathbf{t}) d\\mathbf{w} d\\beta $$\n",
    "\n",
    "Donde:\n",
    "1.  La verosimilitud condicional (el modelo de regresión para una nueva observación $t$ dada una entrada $\\mathbf{x}$) es Gaussiana: $p(t|\\mathbf{x}, \\mathbf{w}, \\beta) = \\mathcal{N}(t|\\mathbf{w}^T\\mathbf{\\phi}(\\mathbf{x}), \\beta^{-1})$.\n",
    "2.  La distribución a posteriori conjunta es Normal-Gamma (resultado del Ejercicio 3.12, que es la forma conjugada de la priori): $p(\\mathbf{w},\\beta|\\mathbf{t})=\\mathcal{N}(\\mathbf{w}|\\mathbf{m}_N,\\beta^{-1}\\mathbf{S}_N)\\text{Gam}(\\beta|a_N, b_N)$.\n",
    "\n",
    "El proceso de integración se realiza en dos pasos: primero se integra $\\mathbf{w}$, y luego se integra $\\beta$.\n",
    "\n",
    "#### Paso 1: Integración de los pesos $\\mathbf{w}$ (obteniendo $p(t|\\mathbf{x}, \\mathbf{t}, \\beta)$)\n",
    "\n",
    "Integramos primero los pesos $\\mathbf{w}$ marginalizando la distribución a posteriori de $\\mathbf{w}$ (condicional en $\\beta$) sobre la verosimilitud:\n",
    "\n",
    "$$ p(t|\\mathbf{x}, \\mathbf{t}, \\beta) = \\int p(t|\\mathbf{x}, \\mathbf{w}, \\beta) p(\\mathbf{w}|\\mathbf{x}, \\mathbf{t}, \\beta) d\\mathbf{w} $$\n",
    "\n",
    "Esta integral es la convolución de dos distribuciones Gaussianas, un caso que surge en modelos lineales-Gaussianos. La distribución resultante $p(t|\\mathbf{x}, \\mathbf{t}, \\beta)$ es también Gaussiana.\n",
    "\n",
    "La distribución predicha para $t$, asumiendo $\\beta$ conocida (similar al resultado (3.58) de la regresión lineal Bayesiana con precisión conocida), es:\n",
    "\n",
    "$$ p(t|\\mathbf{x}, \\mathbf{t}, \\beta) = \\mathcal{N}(t|\\mu_N(\\mathbf{x}), \\beta^{-1} s_N^2(\\mathbf{x})) $$\n",
    "\n",
    "Donde:\n",
    "*   **Media $\\mu_N(\\mathbf{x})$:** Es la media predictiva, dada por el producto del vector de funciones base $\\mathbf{\\phi}(\\mathbf{x})$ (que denotaremos $\\mathbf{\\phi}$) y la media a posteriori de los pesos $\\mathbf{m}_N$:\n",
    "    $$\\mu_N(\\mathbf{x}) = \\mathbf{m}_N^T\\mathbf{\\phi}(\\mathbf{x})$$\n",
    "*   **Varianza $\\beta^{-1} s_N^2(\\mathbf{x})$:** La inversa de $\\beta$ se factoriza, y $s_N^2(\\mathbf{x})$ captura el factor de incertidumbre total que depende de $\\mathbf{x}$ (la suma de la incertidumbre del ruido y la incertidumbre en los parámetros $\\mathbf{w}$):\n",
    "    $$s_N^2(\\mathbf{x}) = 1 + \\mathbf{\\phi}(\\mathbf{x})^T \\mathbf{S}_N \\mathbf{\\phi}(\\mathbf{x})$$\n",
    "\n",
    "#### Paso 2: Integración de la Precisión $\\beta$ (obteniendo $p(t|\\mathbf{x}, \\mathbf{t})$)\n",
    "\n",
    "La distribución predictiva final se obtiene integrando la distribución Gaussiana del Paso 1 con respecto a la distribución marginal posterior de la precisión $\\beta$, que es una distribución Gamma, $\\text{Gam}(\\beta|a_N, b_N)$:\n",
    "\n",
    "$$ p(t|\\mathbf{x}, \\mathbf{t}) = \\int_{0}^{\\infty} \\mathcal{N}(t|\\mu_N(\\mathbf{x}), \\beta^{-1} s_N^2(\\mathbf{x})) \\text{Gam}(\\beta|a_N, b_N) d\\beta $$\n",
    "\n",
    "Esta integral, que marginaliza la precisión (o inversa de la varianza) de una distribución Gaussiana con respecto a una distribución Gamma conjugada, es el resultado canónico que define la distribución $t$ de Student.\n",
    "\n",
    "Formalmente, si integramos una distribución Gaussiana $\\mathcal{N}(x|\\mu, (\\eta\\Lambda)^{-1})$ con respecto a una distribución Gamma $\\text{Gam}(\\eta|\\nu/2, \\nu/2)$, el resultado es una distribución $t$ de Student $\\text{St}(x|\\mu, \\Lambda, \\nu)$.\n",
    "\n",
    "Al realizar esta integración, identificamos los parámetros de la distribución predictiva $p(t|\\mathbf{x}, \\mathbf{t}) = \\text{St}(t|\\mu, \\lambda, \\nu)$ de la siguiente manera:\n",
    "\n",
    "1. Media ($\\mu$):\n",
    "\n",
    "La media de la distribución $t$ de Student es la media del núcleo Gaussiano:\n",
    "$$\\mathbf{\\mu} = \\mu_N(\\mathbf{x}) = \\mathbf{m}_N^T\\mathbf{\\phi}(\\mathbf{x})$$\n",
    "\n",
    "2. Grados de Libertad ($\\nu$):\n",
    "\n",
    "Los grados de libertad $\\nu$ están determinados por el parámetro de forma $a_N$ de la distribución Gamma a posteriori:\n",
    "$$\\mathbf{\\nu} = 2a_N$$\n",
    "(Siempre que $\\nu > 1$, la media está bien definida, y si $\\nu > 2$, la varianza está bien definida).\n",
    "\n",
    "3. Precisión ($\\lambda$):\n",
    "\n",
    "El parámetro de precisión $\\lambda$ (que no es necesariamente la inversa de la varianza) está determinado por una combinación del factor de escala $b_N$ de la Gamma y el factor de incertidumbre espacial $s_N^2(\\mathbf{x})$.\n",
    "\n",
    "La precisión efectiva de la distribución $t$ de Student se calcula como el cociente entre el parámetro de forma de la Gamma ($a_N$) y el producto del parámetro de escala ($b_N$) y el factor de incertidumbre predicha ($s_N^2(\\mathbf{x})$):\n",
    "\n",
    "$$\\mathbf{\\lambda} = \\frac{a_N}{b_N s_N^2(\\mathbf{x})} = \\frac{a_N}{b_N (1 + \\mathbf{\\phi}(\\mathbf{x})^T \\mathbf{S}_N \\mathbf{\\phi}(\\mathbf{x}))}$$\n",
    "\n",
    "Donde los parámetros $\\mathbf{m}_N$, $\\mathbf{S}_N$, $a_N$ y $b_N$ se obtienen del proceso de actualización Bayesiana del conjugado Normal-Gamma (Ejercicio 3.12)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd805fe",
   "metadata": {},
   "source": [
    "### Ejercicio 3.23\n",
    "\n",
    "Demuestre que la probabilidad marginal de los datos, en otras palabras la evidencia del modelo, para el modelo descrito en el ejercicio 3.12 está dada por\n",
    "$$ p(\\mathbf{t}) = \\frac{1}{(2\\pi)^{N/2}} \\frac{b_0^{a_0}}{b_N^{a_N}} \\frac{\\Gamma(a_N)}{\\Gamma(a_0)} \\frac{|\\mathbf{S}_N|^{1/2}}{|\\mathbf{S}_0|^{1/2}} \\tag{3.118}$$\n",
    "\n",
    "primero marginalizando con respecto a $\\mathbf{w}$ y luego con respecto a $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee64c1d",
   "metadata": {},
   "source": [
    "De acuerdo con la (3.10), sabemos que la verosimilitud $p(\\mathbf{t}|\\mathbf{X},\\mathbf{w},\\beta)$ se puede escribir como $p(\\mathbf{t}|\\mathbf{X},\\mathbf{w},\\beta)=\\mathcal{N}(\\mathbf{t}|\\Phi \\mathbf{w},\\beta^{-1}\\mathbf{I})$ y dado que $p(\\mathbf{w}|\\beta)=\\mathcal{N}(\\mathbf{m}_{0},\\beta^{-1}\\mathbf{S_{0}})$ y $p(\\beta)=Gam(\\beta|a_{0},b_{0})$. Por lo tanto, simplemente seguimos la sugerencia del problema.\n",
    "\n",
    "\\begin{align*}\n",
    "p(\\mathbf{t}) &= \\iint p(\\mathbf{t}|\\mathbf{X},\\mathbf{w},\\beta)p(\\mathbf{w}|\\beta)d\\mathbf{w}~p(\\beta)d\\beta\\\\\n",
    "&= \\iint \\left(\\frac{\\beta}{2\\pi}\\right)^{N/2} \\exp\\left\\{-\\frac{\\beta}{2}(\\mathbf{t}-\\Phi \\mathbf{w})^{T}(\\mathbf{t}-\\Phi \\mathbf{w})\\right\\}\\cdot \\\\\n",
    "&\\left(\\frac{\\beta}{2\\pi}\\right)^{M/2}|\\mathbf{S_{0}}|^{-1/2} \\exp\\left\\{-\\frac{\\beta}{2}(\\mathbf{w}-\\mathbf{m}_{0})^{T}\\mathbf{S_{0}}^{-1}(\\mathbf{w}-\\mathbf{m}_{0})\\right\\}d\\mathbf{w}\\\\\n",
    "&\\Gamma(a_{0})^{-1}b_{0}^{a_{0}}\\beta^{a_{0}-1}\\exp(-b_{0}\\beta)d\\beta\\\\\n",
    "&= \\frac{b_{0}^{a_{0}}}{(2\\pi)^{(M+N)/2}|\\mathbf{S_{0}}|^{1/2}} \\iint \\exp\\left\\{-\\frac{\\beta}{2}(t-\\Phi \\mathbf{w})^{T}(t-\\Phi \\mathbf{w})\\right\\}\\cdot\\\\\n",
    "&\\exp\\left\\{-\\frac{\\beta}{2}(\\mathbf{w}-\\mathbf{m}_{0})^{T}\\mathbf{S_{0}}^{-1}(\\mathbf{w}-\\mathbf{m}_{0})\\right\\}d\\mathbf{w}\\\\\n",
    "&\\beta^{a_{0}-1+N/2+M/2}\\exp(-b_{0}\\beta)d\\beta\\\\\n",
    "&= \\frac{b_{0}^{a_{0}}}{(2\\pi)^{(M+N)/2}|\\mathbf{S_{0}}|^{1/2}} \\int \\int \\exp\\left\\{-\\frac{\\beta}{2}(w-\\mathbf{m}_{N})^{T}\\mathbf{S}_{N}^{-1}(w-\\mathbf{m}_{N})\\right\\}d\\mathbf{w} \\\\\n",
    "&\\exp\\left\\{-\\frac{\\beta}{2}(\\mathbf{t}^{T}\\mathbf{t}+{\\mathbf{m}_{0}}^{T}\\mathbf{S_{0}}^{-1}\\mathbf{m}_{0}-{\\mathbf{m}_{N}}^{T}\\mathbf{S}_{N}^{-1}\\mathbf{m}_{N})\\right\\}\\\\\n",
    "&\\beta^{a_{N}-1+M/2}\\exp(-b_{0}\\beta)d\\beta\n",
    "\\end{align*}\n",
    "\n",
    "Donde hemos definido\n",
    "$$ \\mathbf{m_{N}}=\\mathbf{S_{N}}({S_{0}}^{-1}\\mathbf{m}_{0}+\\Phi^{T}\\mathbf{t}) $$\n",
    "$$ \\mathbf{S_{N}}^{-1}=\\mathbf{S_{0}}^{-1}+\\Phi^{T}\\Phi $$\n",
    "$$ a_{N}=a_{0}+\\frac{N}{2} $$\n",
    "$$ b_{N}=b_{0}+\\frac{1}{2}({\\mathbf{m_{0}}}^{T}{\\mathbf{S_{0}}}^{-1}{\\mathbf{m_{0}}}-{\\mathbf{m_{N}}}^{T}{\\mathbf{S_{N}}}^{-1}{\\mathbf{m_{N}}}+\\sum_{n=1}^{N}t_{n}^{2}) $$\n",
    "\n",
    "Que son exactamente los mismos que en el Prob. 3.12, y luego evaluamos la integral, aprovechando la propiedad de normalización de la Distribución Gaussiana multivariada y la Distribución Gamma.\n",
    "\n",
    "\\begin{align*}\n",
    "p(\\mathbf{t}) &= \\frac{b_{0}^{a_{0}}}{(2\\pi)^{(M+N)/2}|\\mathbf{S_{0}}|^{1/2}}\\left(\\frac{2\\pi}{\\beta}\\right)^{M/2}|\\mathbf{S_{N}}|^{1/2}\\int\\beta^{a_{N}-1+M/2}\\exp(-b_{N}\\beta)d\\beta \\\\\n",
    "&=\\frac{b_{0}^{a_{0}}}{(2\\pi)^{(M+N)/2}|\\mathbf{S_{0}}|^{1/2}}(2\\pi)^{M/2}|\\mathbf{S_{N}}|^{1/2}\\int\\beta^{a_{N}-1}\\exp(-b_{N}\\beta)d\\beta \\\\\n",
    "&=\\frac{1}{(2\\pi)^{N/2}}\\frac{|\\mathbf{S_{N}}|^{1/2}}{|\\mathbf{S_{0}}|^{1/2}}\\frac{b_{0}^{a_{0}}}{b_{N}^{a_{N}}}\\frac{\\Gamma(a_{N})}{\\Gamma(b_{N})}\n",
    "\\end{align*}\n",
    "\n",
    "Tal como se requería."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350655e4",
   "metadata": {},
   "source": [
    "### Ejercicio 3.24\n",
    "\n",
    "Repita el ejercicio anterior pero ahora usando el teorema de Bayes en la forma\n",
    "\n",
    "$$ p(\\mathbf{t}) = \\frac{p(\\mathbf{t}|\\mathbf{w}, \\beta)p(\\mathbf{w}, \\beta)}{p(\\mathbf{x}, \\beta | \\mathbf{t})} \\tag{3.119}$$\n",
    "\n",
    "y luego sustituya para las distribuciones a priori y a posteriori y la función de verosimilitud para derivar el resultado (3.118)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0b4fa1",
   "metadata": {},
   "source": [
    "Simplemente sigamos la sugerencia y comencemos escribiendo las expresiones para la verosimilitud y las distribuciones a priori y a posteriori.\n",
    "Sabemos que $p(\\mathbf{t}|\\mathbf{w},\\beta)=\\mathcal{N}(\\mathbf{t}|\\Phi \\mathbf{w},\\beta^{-1}\\mathbf{I})$. Además, la forma de la a priori y la a posteriori son bastante similares:\n",
    "\n",
    "\\begin{align*}\n",
    "p(\\mathbf{w},\\beta)=\\mathcal{N}(\\mathbf{w}|\\mathbf{m}_{0},\\beta^{-1}\\mathbf{S}_{0})\\text{Gam}(\\beta|a_{0},b_{0})\n",
    "\\end{align*}\n",
    "Y\n",
    "\\begin{align*}\n",
    "p(\\mathbf{w},\\beta|\\mathbf{t})=\\mathcal{N}(\\mathbf{w}|\\mathbf{m}_{N},\\beta^{-1}\\mathbf{S}_{N})\\text{Gam}(\\beta|a_{N},b_{N})\n",
    "\\end{align*}\n",
    "\n",
    "Donde las relaciones entre esos parámetros se muestran en los problemas 3.12 y 3.23. Ahora, de acuerdo con (3.119), podemos escribir:\n",
    "\n",
    "\\begin{align*}\n",
    "p(\\mathbf{t})\n",
    "&=\\mathcal{N}(\\mathbf{t}|\\Phi \\mathbf{w},\\beta^{-1}\\mathbf{I})\\frac{\\mathcal{N}(\\mathbf{w}|\\mathbf{m}_{0},\\beta^{-1}\\mathbf{S}_{0})\\text{Gam}(\\beta|a_{0},b_{0})}{\\mathcal{N}(\\mathbf{w}|\\mathbf{m}_{N},\\beta^{-1}\\mathbf{S}_{N})\\text{Gam}(\\beta|a_{N},b_{N})} \\\\\n",
    "&=\\mathcal{N}(\\mathbf{t}|\\Phi \\mathbf{w},\\beta^{-1}\\mathbf{I})\\frac{\\mathcal{N}(\\mathbf{w}|\\mathbf{m}_{0},\\beta^{-1}\\mathbf{S}_{0})}{\\mathcal{N}(\\mathbf{w}|\\mathbf{m}_{N},\\beta^{-1}\\mathbf{S}_{N})}\\frac{b_{0}^{a_{0}}\\beta^{a_{0}-1}\\exp(-b_{0}\\beta)/\\Gamma(a_{0})}{b_{N}^{a_{N}}\\beta^{a_{N}-1}\\exp(-b_{N}\\beta)/\\Gamma(a_{N})} \\\\\n",
    "&=\\mathcal{N}(\\mathbf{t}|\\Phi \\mathbf{w},\\beta^{-1}\\mathbf{I})\\frac{\\mathcal{N}(\\mathbf{w}|\\mathbf{m}_{0},\\beta^{-1}\\mathbf{S}_{0})}{\\mathcal{N}(\\mathbf{w}|\\mathbf{m}_{N},\\beta^{-1}\\mathbf{S}_{N})}\\frac{b_{0}^{a_{0}}}{b_{N}^{a_{N}}}\\frac{\\Gamma(a_{N})}{\\Gamma(a_{0})}\\beta^{a_{0}-a_{N}}\\exp\\{-(b_{0}-b_{N})\\beta\\} \\\\\n",
    "&=\\mathcal{N}(\\mathbf{t}|\\Phi \\mathbf{w},\\beta^{-1}\\mathbf{I})\\frac{\\mathcal{N}(\\mathbf{w}|\\mathbf{m}_{0},\\beta^{-1}\\mathbf{S}_{0})}{\\mathcal{N}(\\mathbf{w}|\\mathbf{m}_{N},\\beta^{-1}\\mathbf{S}_{N})}\\exp\\{-(b_{0}-b_{N})\\beta\\}\\frac{b_{0}^{a_{0}}}{b_{N}^{a_{N}}}\\frac{\\Gamma(a_{N})}{\\Gamma(a_{0})}\\beta^{-N/2}\n",
    "\\end{align*}\n",
    "\n",
    "Donde hemos usado $a_{N}=a_{0}+\\frac{N}{2}$. Ahora tratamos con los términos expresados en forma de Distribución Gaussiana:\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Terminos gaussianos}\n",
    "&=\\mathcal{N}(t|\\Phi w,\\beta^{-1}I)\\frac{\\mathcal{N}(w|m_{0},\\beta^{-1}S_{0})}{\\mathcal{N}(w|m_{N},\\beta^{-1}S_{N})} \\\\\n",
    "&=\\left(\\frac{\\beta}{2\\pi}\\right)^{N/2}\\exp\\left\\{-\\frac{\\beta}{2}(\\mathbf{t}-\\Phi \\mathbf{w})^{T}(\\mathbf{t}-\\Phi \\mathbf{w})\\right\\}\\cdot\\\\\n",
    "&\\frac{|\\beta^{-1}\\mathbf{S_{N}}|^{1/2}}{|\\beta^{-1}\\mathbf{S_{0}}|^{1/2}}\\frac{\\exp\\left\\{-\\frac{\\beta}{2}(\\mathbf{w}-\\mathbf{m_{0}})^{T}\\mathbf{S_{0}}^{-1}(\\mathbf{w}-\\mathbf{m_{0}})\\right\\}}{\\exp\\left\\{-\\frac{\\beta}{2}(\\mathbf{w}-\\mathbf{m_{N}})^{T}\\mathbf{S_{N}}^{-1}(\\mathbf{w}-\\mathbf{m_{N}})\\right\\}} \\\\\n",
    "&=\\left(\\frac{\\beta}{2\\pi}\\right)^{N/2}\\frac{|\\mathbf{S_{N}}|^{1/2}}{|\\mathbf{S_{0}}|^{1/2}}\\exp\\left\\{-\\frac{\\beta}{2}(\\mathbf{t}-\\Phi \\mathbf{w})^{T}(\\mathbf{t}-\\Phi \\mathbf{w})\\right\\}\\cdot\\\\\n",
    "&\\frac{\\exp\\left\\{-\\frac{\\beta}{2}(\\mathbf{w}-\\mathbf{m_{0}})^{T}\\mathbf{S_{0}}^{-1}(\\mathbf{w}-\\mathbf{m_{0}})\\right\\}}{\\exp\\left\\{-\\frac{\\beta}{2}(\\mathbf{w}-\\mathbf{m_{N}})^{T}\\mathbf{S_{N}}^{-1}(\\mathbf{w}-\\mathbf{m_{N}})\\right\\}}\n",
    "\\end{align*}\n",
    "\n",
    "Si miramos el problema anterior, notamos que en el último paso de la deducción de $p(\\mathbf{t})$, completamos el cuadrado con respecto a $\\mathbf{w}$. \n",
    "Y si comparamos cuidadosamente el lado izquierdo y el derecho en el último paso, podemos obtener:\n",
    "\n",
    "\\begin{align*}\n",
    "&\\exp\\left\\{-\\frac{\\beta}{2}(\\mathbf{t}-\\Phi \\mathbf{w})^{T}(\\mathbf{t}-\\Phi \\mathbf{w})\\right\\}\\exp\\left\\{-\\frac{\\beta}{2}(\\mathbf{w}-\\mathbf{m_{0}})^{T}\\mathbf{S_{0}}^{-1}(\\mathbf{w}-\\mathbf{m_{0}})\\right\\} \\\\\n",
    "=&\\exp\\left\\{-\\frac{\\beta}{2}(\\mathbf{w}-\\mathbf{m_{N}})^{T}\\mathbf{S_{N}}^{-1}(\\mathbf{w}-\\mathbf{m_{N}})\\right\\}\\exp\\{-(b_{N}-b_{0})\\beta\\}\n",
    "\\end{align*}\n",
    "\n",
    "Por lo tanto, volvemos a tratar con los términos Gaussianos:\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{Términos Gaussianos} = \\left(\\frac{\\beta}{2\\pi}\\right)^{N/2}\\frac{|\\mathbf{S_{N}}|^{1/2}}{|\\mathbf{S_{0}}|^{1/2}}\\exp\\{-(b_{N}-b_{0})\\beta\\}\n",
    "$$\n",
    "\n",
    "Si sustituimos las expresiones anteriores en $p(\\mathbf{t})$, obtendremos (3.118) inmediatamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70585222",
   "metadata": {},
   "source": [
    "## Parte II\n",
    "### Ejercicio 2.1. \n",
    "A partir de 500 pares de variables entrada-salida, codificar un programa que prediga las salidas para nuevas variables de entrada donde las salidas deseadas son desconocidas. Elegir un conjunto |400| para ajuste y uno |100| para validación. Elegir las funciones de base y construir la matriz del modelo $\\Phi$. Obtener el vector de coeficientes resolviendo $\\Phi^T\\Phi \\mathbf{w}=\\Phi^T\\mathbf{t}$. A partir de los valores de validación, predecir los $t_n$ mediante\n",
    "$$\\hat{t} = y(x, \\mathbf{w})= \\mathbf{w}^T\\Phi(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70532046",
   "metadata": {},
   "source": [
    "#### 1. Partición del Conjunto de Datos (500 pares entrada-salida)\n",
    "\n",
    "Se comienza con $N=500$ pares de datos $\\{\\mathbf{x}_n, t_n\\}$.\n",
    "\n",
    "* Conjunto de Ajuste (Entrenamiento): Se utiliza el conjunto de 400 pares de datos (denotado como el conjunto de entrenamiento) para determinar los coeficientes $\\mathbf{w}$.\n",
    "* Conjunto de Validación: Se utiliza un conjunto separado de 100 pares de datos (también llamado *hold-out set* o *development set*) para evaluar la capacidad predictiva del modelo en datos no vistos y seleccionar la complejidad adecuada (selección de modelo).\n",
    "\n",
    "El uso de un conjunto de validación es esencial, ya que el rendimiento en el conjunto de entrenamiento no es un buen indicador del rendimiento predictivo en datos no vistos debido al problema de sobreajuste (over-fitting).\n",
    "\n",
    "#### 2. Elección de las Funciones de Base y Formulación del Modelo\n",
    "\n",
    "El modelo de predicción, $y(\\mathbf{x}, \\mathbf{w})$, se construye como una combinación lineal de funciones no lineales fijas de las variables de entrada, conocidas como funciones de base $\\phi_j(\\mathbf{x})$.\n",
    "\n",
    "El modelo toma la forma:\n",
    "$$y(\\mathbf{x}, \\mathbf{w}) = w_0 + \\sum_{j=1}^{M-1} w_j \\phi_j(\\mathbf{x}) = \\mathbf{w}^T\\boldsymbol{\\phi}(\\mathbf{x})$$\n",
    "\n",
    "Donde:\n",
    "*   $\\mathbf{w} = (w_0, \\dots, w_{M-1})^T$ es el vector de coeficientes.\n",
    "*   $\\boldsymbol{\\phi}(\\mathbf{x}) = (\\phi_0(\\mathbf{x}), \\dots, \\phi_{M-1}(\\mathbf{x}))^T$ es el vector de funciones de base, incluyendo una función de base ficticia $\\phi_0(\\mathbf{x})=1$ para el término de sesgo (bias) $w_0$.\n",
    "*   $M$ es el número total de parámetros (coeficientes).\n",
    "\n",
    "Las funciones de base pueden ser, por ejemplo, polinomiales, Gaussianas o sigmoidales. La elección de $M$ (el orden del polinomio o el número de funciones de base) es crucial, ya que controla la complejidad del modelo (model complexity).\n",
    "\n",
    "#### 3. Construcción de la Matriz del Modelo $\\Phi$ (Matriz de Diseño)\n",
    "\n",
    "La matriz de diseño $\\Phi$ es una matriz $N \\times M$ (donde $N=400$ es el número de puntos de ajuste y $M$ es el número de parámetros).\n",
    "\n",
    "Los elementos de $\\Phi$ están dados por $\\Phi_{nj} = \\phi_j(\\mathbf{x}_n)$, donde $n=1, \\dots, N$ (los 400 puntos de ajuste) y $j=0, \\dots, M-1$:\n",
    "\n",
    "$$\\Phi = \\begin{pmatrix} \\phi_0(\\mathbf{x}_1) & \\phi_1(\\mathbf{x}_1) & \\cdots & \\phi_{M-1}(\\mathbf{x}_1) \\\\ \\phi_0(\\mathbf{x}_2) & \\phi_1(\\mathbf{x}_2) & \\cdots & \\phi_{M-1}(\\mathbf{x}_2) \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\phi_0(\\mathbf{x}_N) & \\phi_1(\\mathbf{x}_N) & \\cdots & \\phi_{M-1}(\\mathbf{x}_N) \\end{pmatrix}$$\n",
    "\n",
    "#### 4. Obtención del Vector de Coeficientes $\\mathbf{w}$\n",
    "\n",
    "Para obtener los coeficientes $\\mathbf{w}$, se minimiza una función de error que mide el desajuste entre las predicciones $y(\\mathbf{x}_n, \\mathbf{w})$ y los valores objetivo $t_n$ del conjunto de ajuste.\n",
    "\n",
    "Una elección común y simple es la función de error de suma de cuadrados (sum-of-squares error function):\n",
    "$$E(\\mathbf{w}) = \\frac{1}{2} \\sum_{n=1}^{N} \\{y(\\mathbf{x}_n, \\mathbf{w}) - t_n\\}^2$$\n",
    "\n",
    "La minimización de esta función de error se logra resolviendo el sistema de ecuaciones lineales conocido como las ecuaciones normales.\n",
    "\n",
    "El vector de coeficientes $\\mathbf{w}$ que minimiza $E(\\mathbf{w})$ está dado por la solución de:\n",
    "$$\\mathbf{\\Phi}^T\\mathbf{\\Phi} \\mathbf{w}=\\mathbf{\\Phi}^T\\mathbf{t}$$\n",
    "\n",
    "La solución de mínimos cuadrados (Maximum Likelihood solution, $\\mathbf{w}_{ML}$) se puede expresar en forma cerrada (closed form):\n",
    "$$\\mathbf{w} = (\\mathbf{\\Phi}^T\\mathbf{\\Phi})^{-1} \\mathbf{\\Phi}^T\\mathbf{t}$$\n",
    "\n",
    "*Nota:* La función de error de suma de cuadrados surge de maximizar la verosimilitud (maximum likelihood) bajo el supuesto de que el ruido es una distribución Gaussiana.\n",
    "\n",
    "#### 5. Predicción de Nuevas Salidas $\\hat{t}$ (Validación)\n",
    "\n",
    "Una vez que se obtiene el vector de coeficientes óptimo $\\mathbf{w}$, el modelo puede usarse para predecir el valor de $t$ para cualquier nueva entrada $\\mathbf{x}$ (por ejemplo, los 100 puntos del conjunto de validación).\n",
    "\n",
    "La predicción $\\hat{t}$ para un nuevo punto $\\mathbf{x}$ se calcula mediante:\n",
    "$$\\hat{t} = y(\\mathbf{x}, \\mathbf{w})= \\mathbf{w}^T\\mathbf{\\Phi}(\\mathbf{x})$$\n",
    "\n",
    "Aquí, $\\mathbf{\\Phi}(\\mathbf{x})$ representa el vector de las funciones de base evaluadas en el nuevo punto $\\mathbf{x}$. La evaluación de esta predicción sobre el conjunto de validación permite cuantificar la generalización del modelo (el objetivo clave en machine learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20746415",
   "metadata": {},
   "source": [
    "### Ejercicio 2.2. \n",
    "Usar al menos dos funciones de base, y elegir la de mejor desempeño."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297a4f3f",
   "metadata": {},
   "source": [
    "### Ejercicio 2.3.\n",
    "\n",
    "Determinar un modo de medir el sobre-ajuste y plotear la métrica de sesgo-varianza."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eba1ef0",
   "metadata": {},
   "source": [
    "### Ejercicio 2.4.\n",
    "\n",
    "Representar $p(D|x,\\mathbf{w})$ en gráfica para cada caso."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
